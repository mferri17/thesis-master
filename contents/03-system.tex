\chapter{System Description}
\label{chap:system}

\glsreset{idsia}

This chapter aims to provide a generic view of our system. First, we briefly describe the existing environment, its main components and how they interact for flying and controlling the drone. Next, we list tools, software and libraries used to achieve the goal of making the drone able to fly in any other environment. %, as explained in the introductive section \ref{sec:objective}.




\section{Environment}
\label{sec:hardware}

Since we mainly focus on improving the ProximityNet model described in section \ref{subsec:proximitynet}, we need to know the physical environment in which the original research has been conducted, located at the Swiss AI Lab \gls{idsia} in Lugano.



\subsection{Parrot Bebop Drone 2}
\label{subsec:bebop}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=.8\textwidth]{"contents/images/03-Parrot-Bebop-2"}
	\caption[Parrot Bebop Drone 2]{Parrot Bebop Drone 2}
	\label{fig:bebop}
\end{figure}

The entire work is built around the Parrot Bebop Drone 2 (figure \ref{fig:bebop}), a lightweight drone (500 grams) with size of $382 \times 328 \times 89$ millimeters. A 2700 mAh swappable battery gives power to four brushless engines and dual-core processor with quad-core GPU for a maximum flight time of 25 minutes. Connectivity is provided through 2.4 GHz 802.11a/b/n/ac Wi-Fi that enables remote control via mobile app or Parrot Skycontroller (up to a distance of 2km).


The drone is equipped with many simultaneous sensor to compute drone's velocities, orientation, altitude, attitude and GPS coordinates to ensure the maximum stability during the whole flight. However, for this project we mainly care about its camera, able to shoot 14 \gls{mp} photos and record Full HD 1080p videos at 30 \gls{fps}. Even though the original \gls{fov} is 180°, raw camera images pass through a software stabilization that produces 16:9 images with a horizontal \gls{fov} of 90°. The 3-axis digital stabilization technique implemented by Parrot is able to compensate for drone's pitch and roll, in order to provide correct-oriented horizontal images and stable videos regardless the drone's movements. Full specifications provided by the official \cite{bebop}.

  

\subsection{OptiTrack}
\label{subsec:optitrack}

For tracking drone's movement a \gls{mocap} system is required, able to record 3D coordinate of objects and people in space. The technique is widely use for motion tracking in a large variety of fields such as film making and animation, virtual reality, sport, medicine and even military. A common way to implement a \gls{mocap} systems is by using special cameras placed around the area to be tracked, able to collect optical signals from passive\footnote{a passive marker reflect light} or active markers\footnote{an active marker emits its own light} inside the area.

\medskip

\gls{idsia} adopt OptiTrack, which is producing real-time \gls{mocap} systems since 1996 and are the today world’s choice for low-latency and high-precision 6 \gls{dof} tracking for ground and aerial robotics both indoor and outdoor. Full documentation is available on the \cite{optitrack}.



\subsection{Drone Arena}
\label{subsec:drone-arena}

At \gls{idsia}, a dedicated room has been equipped with an OptiTrack \gls{mocap} system composed by 12 OptiTrack Prime$^x$13 \gls{ir} cameras for medium-sized areas (figure \ref{fig:optitrack-camera}, 1.3 \gls{mp}, 240 \gls{fps}, $\pm0.20$ mm 3D accuracy in a $9 \times 9$ meters area with 14mm marker), to track movements of passive markers placed on the person's head facing the drone and on the drone itself. Schematic and actual representation of the arena are shown in figures \ref{fig:optitrack-schema} and \ref{fig:drone-arena}. Such composition is able to track a theoretical number of 18 drones inside an available area of $6 \times 6$ meters (here surrounded by a safety net), with a virtual fence of $4.8 \times 4.8$ meters which virtually constraints the total area in which the drone is allowed to fly.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=1\textwidth]{"contents/images/03-optitrack-schema"}
	\caption[Schematic OptiTrack system with 12 OptiTrack Prime cameras]{Schematic OptiTrack system with 12 OptiTrack Prime cameras}
	\label{fig:optitrack-schema}
\end{figure}

\begin{figure}[!htb]
	\begin{center}
		\begin{subfigure}[h]{0.29\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/03-optitrack-camera"}
			\caption[]{Prime$^x$13 camera}
			\label{fig:optitrack-camera}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.69\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/03-arena"}
			\caption[]{Drone in front of the cameras with passive markers}
			\label{fig:drone-arena}
		\end{subfigure}
	\end{center}
	\vspace{-0.5cm}
	\caption[Drone arena at \gls{idsia}]{Drone arena at \gls{idsia}}
\end{figure}



\subsection{Robot Operating System (ROS)}
\label{subsec:ROS}

\gls{ros} is an open-source robotics middleware suite of software libraries and tools for building distributed and modular robot applications. It provides hardware astraction and orchestration, implementation of commonly used functionality, message-passing between processes, and package management. \gls{ros} organizes its components in graph architecture composed by nodes which communicates via a publish/subscribe mechanism, supporting a wide variety of robots also used for education. The main client library is available in C++, Python and Lisp.

Some of the most important \gls{ros} features include Standard Message Definitions, Robot Geometry and Description, Remote Procedure Calls, Diagnostics, Pose Estimation, Localization, Mapping, and Navigation. It also provides additional tools, such as \cite{rviz} (3D visualization of robots and various types of sensor data) and \cite{Gazebo} (3D indoor and outdoor multi-robot simulator, complete with dynamic and kinematic physics, and a pluggable physics engine).

\medskip

\gls{ros} has grown to include a large community of active users worldwide. Historically, the majority of the users were in research labs, but increasingly we are seeing adoption in the commercial sector, particularly in industrial and service robotics.

Further documentation is available on the official \cite{ROS}.



\subsection{Control \& Data collection}
\label{subsec:control}

Inside the arena, the drone is controlled by a \gls{ros} script which relies on the user's pose \gls{wrt} the drone - from now on, the \textit{target pose} (i.e., the pose of the user seen by the drone reference frame) - to compute acceleration commands for making the drone hover in front of the person, in the direction of the head at a predefined 1.5 meters distance. 

\medskip

During data collection, both user's and drone's poses are computed by the OptiTrack system by using proper markers placed on the drone and on the person's head, as shown in picture \ref{fig:drone-facing}. The target poses over time, mathematically computed by the script, are accurately synchronized with the video stream from the front-facing camera and saved into \texttt{rosbag} files.

These data are used to build the dataset for training a machine learning model, which should be able to infer the target pose by seeing a picture taken by the drone's camera. Figure \ref{fig:drone-demo-2} shows an illustration of the system from a bird-eye view.

\medskip

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.8\textwidth]{"contents/images/03-drone-facing"}
	\caption[Passive markers placed on top of drone and user's head]{Passive markers placed on top of drone and user's head}
	\label{fig:drone-facing}
\end{figure}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=1\textwidth]{"contents/images/03-arena-demo-2"}
	\caption[Data collection in the drone arena through OptiTrack and video stream]{Data collection in the drone arena through OptiTrack and video stream}
	\label{fig:drone-demo-2}
\end{figure}




\section{Development}
\label{sec:software}

This section presents tools and software used to conduct our research for improving the existing system from a machine learning point of view, according to the objective of the thesis.



\subsection{Tools}
\label{subsec:tools}

The entire source code is written in Python 3. First experiments were carried out with Jupyter Notebooks via Google Colab on a GPU-accelerated runtime, while the final code is provided as classic Python scripts to be executable on a custom machine.

For debugging a Windows 10 laptop equipped with an NVIDIA GeForce GTX 950M graphic card has been used, while actual training has been performed on a dedicated Ubuntu 18.04 workstation available at \gls{idsia} mounting four NVIDIA GeForce RTX 2080 Ti (still used for single-GPU computing, not simultaneously).



\subsection{Frameworks}
\label{subsec:frameworks}

The original work from \cite{mantegazza2019visionbased} is written in Python and based on TensorFlow 1 and Keras. These libraries have been kept, but our project uses their updated versions for ease of use. Other intensively used frameworks are listed below.


\paragraph*{\texttt{Numpy}}
Largely used in the whole project for computation on arrays. Numpy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays. 

\paragraph*{\texttt{Pickle}}
Mainly used for saving and restoring Numpy arrays. The pickle module implements binary protocols for serializing and de-serializing a Python object structure. 

\paragraph*{\texttt{Matplotlib}}
First choice for building charts, visualize images or any kind of figure. Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. Its \texttt{pyplot} module is inspired by MATLAB.

\paragraph*{\texttt{OpenCV}}
Mainly used for efficient image and video manipulation together with Matplotlib. OpenCV is an open-source library that includes several hundreds of computer vision algorithms.

\paragraph*{\texttt{TensorFlow 2}}
All the project strongly relies on TensorFlow (TF) from start to end: network interpretation, person masking, training and quantitative evaluation. Created by the Google Brain team, TensorFlow is an open source library for numerical computation and large-scale machine learning. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks. In version 2, it introduces a lot of comforts for easier development with a less steep learning curve.

\paragraph*{\texttt{Keras}}
Used for defining the network architecture, training and evaluating the model. Keras is the high-level API of TensorFlow 2: an approchable, highly-productive interface for solving machine learning problems, with a focus on modern deep learning. It provides essential abstractions and building blocks for developing and shipping machine learning solutions with high iteration velocity.

\paragraph*{\texttt{TensorBoard}}
Used with TensorFlow to precisely profile data generator performance for optimizing training time. TensorBoard is a tool for providing the measurements and visualizations needed during the machine learning workflow. It enables tracking experiment metrics, visualizing the model graph, and much more.

\paragraph*{\texttt{Sklearn}}
Only used for automatically compute some evaluation metrics, Sklearn is a simple and efficient tools for predictive data analysis reusable in various contexts built on NumPy, SciPy, and Matplotlib.

\paragraph*{\texttt{tf-keras-vis}}
Used for applying GradCAM and other interpretability techniques. Open-source library for network interpretation, available on GitHub thanks to \cite{tf-keras-vis}. Derived from the original keras-vis (\cite{keras-vis}) high-level toolkit for visualizing and debugging your trained keras neural net models.

\paragraph*{\texttt{akTwelve Mask\_RCNN}}
Used for human detection and segmentation in background replacement. Open-source implementation of Mask R-CNN on Python 3, Keras, and TensorFlow available on GitHub thanks to \cite{MaskRCNN_akTwelve}. The model generates bounding boxes and segmentation masks for each instance of an object in the image. It's based on Feature Pyramid Network (FPN) and a ResNet101 backbone.







