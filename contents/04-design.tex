\chapter{Solution Design}
\label{chap:design}

\glsreset{ai}
\glsreset{nn}
\glsreset{cnn}

This chapter explores the issues of the existing model, proposes a solution, and presents initial experiments on its feasibility.




\section{Problem Summary}
\label{sec:frontalnet-generalization}

In section \ref{subsec:sota-dario}, we introduce the original paper we are working on and present its architecture and basic performance. Chapter \ref{chap:system} illustrates the main components for controlling the drone and the dataset used to train the machine learning model, as declared in \cite{mantegazza2019visionbased}.

\medskip

FrontalNet achieved quite good performance on the test set, as explained in section \ref{subsec:frontalnet-performance}. However, its behavior must be proven on the real drone to certify the model usability. \cite{mantegazza2019visionbased} reports experiments conducted inside the arena by flying the drone without the \gls{mocap} system, only relying on the learned model for computing the user's pose. The outcome is incredibly good, with the drone actually performing its task without any issues\footnote{see figure \ref{fig:frontalnet-trajectories} in the appendix for further details}.

Finally, we consider model performance in unknown environments, possibly outdoor. The official paper does not talk about the topic but during a direct discussion with the author, we discovered that flying performance outside of the drone arena was not consistent with usual model behavior. The drone was not able to follow the user and its movements were unexplainable. We conclude that the model is not able to generalize the task when outside of the environment it already knows.

\medskip

Our goal is to explore ways of improvement, to generalize the model and make it able to theoretically predict the user's pose in any other unknown scenario. The next sections firstly try to understand the main issues and limitations of the model, then provide a possible solution for the generalization problem.




\section{Model Interpretation with Grad-CAM}
\label{sec:model-interpretration}

\glsreset{nn}
\glsreset{gradcam}

In the previous section, we discussed insufficient experimental results obtained by FrontalNet in predicting the user's pose in an unknown environment (i.e., outside of the drone arena). This section highlights the main issues behind the lack of its generalization capabilities, by understanding what the model is actually learning.

\medskip

Convolutional Neural Networks (CNN) are suited for computer vision thanks to their ability to gain spatial-related insights from images. As any other \gls{nn}, even \gls{cnn}s are "black-boxes". This means that their internal behavior is particularly challenging for humans to understand.

Among network interpretability tecnhiques introduced in section \ref{sec:network-interpretability}, we choose \gls{gradcam}. The algorithm is indeed the most understandable way of visualizing what a \gls{cnn} is actually seeing.

As explained in section \ref{subsec:gradcam-theory}, \gls{gradcam} is able to effectively visualize the most important parts of an input image which are actually responsible for predicting a certain output\footnote{for an easy understandable \gls{gradcam} example, please refer to the figure \ref{fig:gradcam-catdog} which regards a simple dogs VS cats classifier}.

\medskip

Research in the field is still on-going and most of the available resources are for TensorFlow 1. The most powerful and famous library for network interpretability is \texttt{Lucid}\footnote{\url{https://github.com/tensorflow/lucid}}, from the official TensorFlow team, but it is not easily applicable to Keras models.

Then, we use the open-source library \texttt{tf-keras-vis} by \cite{tf-keras-vis}, porting for TensorFlow 2.0+ of \texttt{keras-vis} by \cite{keras-vis}.



\subsection{Regression to Classification}
\label{subsec:gradcam-regrtoclass}

\gls{gradcam} is designed to be applied on classification tasks, rather than regression ones. Even though a porting of the algorithm for regression has been published (Regression Activation Map, \cite{wang2019diabetic}), it appears to be an isolated case. For this reason, and for network interpretation only, we decide to transform our problem into a classification task.

The ground truth is composed of four variables with specific domains, and figure \ref{fig:frontalnet-dataset-distribution-regr} in the previous chapter shows their distribution. Every variable has a particular "central" value, which is obtained when the user is somehow centered in the image. We decide to split continuous values into 3 different classes, which account for values smaller, around, and higher than the "center". We call these buckets respectively \texttt{low}, \texttt{medium}, and \texttt{high}.

\begin{itemize}
	\item \texttt{X} values are splitted at $1.4$ and $1.6$
	\item \texttt{Y} values are splitted at $-0.15$ and $+0.15$
	\item \texttt{Z} values are splitted at $-0.05$ and $+0.05$
	\item \texttt{W} values are splitted at $-0.20$ and $+0.20$
\end{itemize}

So, for example, \texttt{X} values greater than $1.6$ will be classified as \texttt{high}, while \texttt{Z} values between $-0.05$ and $+0.05$ will be classified as \texttt{medium}. These specific intervals have been manually defined for obtaining a good class distribution over the training set (figure \ref{fig:frontalnet-dataset-distribution-class}). This class partition is fundamental for understanding some \gls{gradcam} visualizations later.

\begin{figure}[!h]
	\begin{center}
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/distributions/x-class"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/distributions/y-class"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/distributions/z-class"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/distributions/w-class"}
		\end{subfigure}
	\end{center}
	\vspace{-0.5cm}
	\caption[Target variables distribution for the classification task]{Target variables distribution for the classification task}
	\label{fig:frontalnet-dataset-distribution-class}
\end{figure}



\subsection{Re-training}
\label{subsec:gradcam-retrain}

Considering the new ground truth, we define a new model architecture by replacing regression outputs with classification ones. 
%Early layers of a \gls{cnn} focus on learning simple features, such as edges and textures, while later convolutional layers usually recognize parts or complete objects. For this reason, we perform a model re-training by freezing most of the layers, learning new model parameters from just from the very last convolutional layer.
We perform a re-training on the new model, by using the \texttt{categorical\_crossentropy} loss and the \texttt{accuracy} metric, both suited for multi-class models. We use the \gls{adam} optimizer and a base learning rate of $0.001$, progressively reduced on validation loss plateaus. 

Results are shown in figure \ref{fig:gradcam-retrain-simple}, which after 30 epochs shows a loss slightly smaller than 1, both for training and validation, and accuracy over the 80\% for all variables. These values are not ideal for gaining some sort of conclusion, but have instead been used for comparing different training experiments conducted on the new classification model.

\begin{figure}[!h]
	\centering
	\includegraphics[width=1\textwidth]{"contents/images/04-metrics-class-simple"}
	\caption[\gls{gradcam}: loss and accuracy of the new classification model]{\gls{gradcam}: loss and accuracy of the new classification model}
	\label{fig:gradcam-retrain-simple}
\end{figure}



\subsection{Reading charts}
\label{subsec:gradcam-reading}

A proper understanding of this section requires a thorough ability on reading the following charts\footnote{Basic knowledge on how the related library works would also be helpful. Tutorial:\url{https://github.com/keisen/tf-keras-vis/blob/8f83773520069367902becc0a668dda90ab76349/examples/attentions.ipynb}}.

\gls{gradcam} application requires to specify the class for which we want to compute the activation mapping. In our case, it would be one of the 3 defined in section \ref{subsec:gradcam-regrtoclass}: \texttt{low}, \texttt{medium} or \texttt{high}. 

When working on standard classification problems, things are pretty easy: if we classify animals, we indicate \gls{gradcam} a specific animal class (e.g., \texttt{lion}), and the algorithm will provide a heatmap that overlay the portion of the image which is mainly associated with that animal (e.g., hopefully, the lion will be highlighted if it is present in the image).

\medskip

However, our \gls{ml} does not look like a standard classification problem, since it has been adapted from regression. Classes only serve as categorical values for variables that are actually numerical; also, does not exist a specific portion for each input image that can be associated with one of the three classes in particular. In other words, considering the class \texttt{low}, we do not expect its related heatmap to be different from the one generated for the class \texttt{high}. Our discriminator is always the person and its position in the image.

Moreover, FrontalNet predicts four different variables, then also the classification model will have multiple outputs. This further introduces another complexity on reading \gls{gradcam} results, since we expect - once again - that heatmaps will only highlight the person in the image, regardless of the inspected variable.

Finally, in some cases, the network predicts the right class (coherent with the \gls{gt}), while in other cases the outputs are incorrect. When examining wrong predictions, both the predicted and actual class have to be taken into consideration for understanding what is going wrong with the model.

Of course, this assumptions can be wrong, as the model could actually use other parts of the images (e.g., objects in the background) for actually determining classes to predict. However, this would be an undesired behavior since we expect that a correctly working \gls{cnn} will only care about the person and nothing else.

\bigskip

Figure \ref{fig:gradcam-example-1} displays a typical example on \gls{gradcam} application for each variable (\texttt{X, Y, Z, W}) and each class (\texttt{low, medium, high}). Heatmaps are not easy to interpret due to a large variety of parameters to consider. As a guideline, for each variable, you could only consider the column which corresponds to the actual (or predicted) class.

\begin{figure}[!h]
	\centering
	\includegraphics[width=1\textwidth]{"contents/images/gradcam/04-gradcam-example-1"}
	\caption[\gls{gradcam}: example of application for each variable and class]{\gls{gradcam}: example of application for each variable and class}
	\label{fig:gradcam-example-1}
\end{figure}

\gls{gt} and predicted values are available in the right-most parenthesis, as "GT" and "PR" respectively. Rows define variables, while columns stand for the classes. It is clearly visible how no specific correlation is available between variables, classes, and computed predictions.

The last column is available as an average result of all classes, obtained by calling \gls{gradcam} without specifying a particular class to consider. The same reasoning can be also applied for variables, so that we are able to obtain also single meaningful images which represent \gls{gradcam} global average, for every variable and class at once (figure \ref{fig:gradcam-example-2}).

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.5\textwidth]{"contents/images/gradcam/04-gradcam-example-2"}
	\caption[\gls{gradcam}: example of application on a global average]{\gls{gradcam}: example of application on a global average}
	\label{fig:gradcam-example-2}
\end{figure}



\subsection{Results}
\label{subsec:gradcam-results}

As already shown in figure \ref{fig:gradcam-example-1}, it seems the network is not only considering the person in the frame for computing its output but instead relies on the whole image with particular attention on some spots.

From the previous section, we understand that reasoning with \gls{gradcam} heatmaps is not trivial, and separating visualization by variables and classes is not totally convenient when we can simply plot the global average \gls{gradcam} instead. For simplicity, this section will only focus on single-image results, while full \gls{gradcam} visualizations are available in the appendix \ref{sec:extra-gradcam} for further inspection.

\subsubsection*{Reasonable detections}

First of all, figure \ref{fig:gradcam-ok} displays examples of correctly working scenarios, in which the person is correctly detected from \gls{gradcam}. Three cases are observed during our studies: most of the times, the entire user is highlighted by \gls{gradcam}, while sometimes just the body or the head get major attention.

\begin{figure}[!h]
	\begin{center}
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-ok-1"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-ok-2"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-ok-3"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-ok-4"}
		\end{subfigure}
	\end{center}
	\vspace{-0.5cm}
	\caption[\gls{gradcam}: Correctly detected people]{\gls{gradcam}: Correctly detected people}
	\label{fig:gradcam-ok}
\end{figure}

Such precise results are not the standard. In many cases, heatmaps are unstable, going in and out of the target person. The two frame sequences below fairly describe the usual behavior of the model seen by \gls{gradcam} (figures \ref{fig:gradcam-seq-dario} and \ref{fig:gradcam-seq-beard}).

\begin{figure}[!h]
	\begin{center}
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-dario-1"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-dario-2"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-dario-3"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-dario-4"}
		\end{subfigure}
	\end{center}
	\vspace{-0.5cm}
	\caption[\gls{gradcam}: Sequence transitioning from wrong to correct detections]{\gls{gradcam}: Sequence transitioning from wrong to correct detections}
	\label{fig:gradcam-seq-dario}
\end{figure}

\begin{figure}[!h]
	\begin{center}
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-beard-01"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-beard-02"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-beard-03"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-beard-04"}
		\end{subfigure}
		\vfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-beard-05"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-beard-06"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-beard-07"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-beard-08"}
		\end{subfigure}
		\vfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-beard-09"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-beard-10"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-beard-11"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-beard-12"}
		\end{subfigure}
	\end{center}
	\vspace{-0.5cm}
	\caption[\gls{gradcam}: Sequence of unstable detections in and out of the person]{\gls{gradcam}: Sequence of unstable detections in and out of the person}
	\label{fig:gradcam-seq-beard}
\end{figure}

\subsubsection*{Problematic detections}

The examples presented above mostly reflect the model expected behavior. However, our network interpretation also reveals a lot of flaws in the prediction task. \gls{gradcam} exhibits several situations in which the model output is affected by recurrent elements in the dataset.

\begin{itemize}
	\item Objects in the background are prone to be considered important (figure \ref{fig:gradcam-background})
	\item Curtains seem often particularly attractive (figure \ref{fig:gradcam-curtains})
	\item Many parts of the room can easily distract the model, such as borders and baseboards or even blank spots on the walls (figure \ref{fig:gradcam-random})
	\item When dealing with multiple people in front of the camera, sometimes not only the nearest person is considered (figure \ref{fig:gradcam-double})
	\item Artificial glitches are sometimes ignored, sometimes distractive (figure \ref{fig:gradcam-glitch})
\end{itemize}

\begin{figure}[!h]
	\begin{center}
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-background-1"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-background-2"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-background-3"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-background-4"}
		\end{subfigure}
	\end{center}
	\vspace{-0.5cm}
	\caption[\gls{gradcam}: Objects in the background detected]{\gls{gradcam}: Objects in the background detected}
	\label{fig:gradcam-background}
\end{figure}

\begin{figure}[!h]
	\begin{center}
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-curtains-1"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-curtains-2"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-curtains-3"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-curtains-4"}
		\end{subfigure}
	\end{center}
	\vspace{-0.5cm}
	\caption[\gls{gradcam}: Curtains often distract the model]{\gls{gradcam}: Curtains often distract the model}
	\label{fig:gradcam-curtains}
\end{figure}

\begin{figure}[!h]
	\begin{center}
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-random-1"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-random-2"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-random-3"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-random-4"}
		\end{subfigure}
	\end{center}
	\vspace{-0.5cm}
	\caption[\gls{gradcam}: Model get easily distracted by various elements]{\gls{gradcam}: Model get easily distracted by various elements}
	\label{fig:gradcam-random}
\end{figure}

\begin{figure}[!h]
	\begin{center}
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-double-1"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-double-2"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-double-3"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-double-4"}
		\end{subfigure}
	\end{center}
	\vspace{-0.5cm}
	\caption[\gls{gradcam}: Detections when two people are present in the image]{\gls{gradcam}: Detections when two people are present in the image}
	\label{fig:gradcam-double}
\end{figure}

\begin{figure}[!h]
	\begin{center}
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-glitch-1"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-glitch-2"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-glitch-3"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/gradcam/gradcam-glitch-4"}
		\end{subfigure}
	\end{center}
	\vspace{-0.5cm}
	\caption[\gls{gradcam}: Model reactions to artificial glitches]{\gls{gradcam}: Model reactions to artificial glitches}
	\label{fig:gradcam-glitch}
\end{figure}



\subsection{Summary}
\label{subsec:gradcam-summary}

Reported results demonstrate that the model is not robust enough to only focus on the user who is actually facing the drone's camera. Instead, various portions of the input images appear to be taken into consideration when the model makes its predictions: many distractors are coming from the background.

In light of this, we can reasonably assume that the \gls{resnet} model has undesirably learned some details about the drone arena in which the dataset has been collected. This is most likely the reason why the model is unable to control the drone outside of that environment, as discussed in section \ref{sec:frontalnet-generalization}.




\section{Person Masking}
\label{sec:masking}

From \gls{gradcam} results presented in the previous section, we conclude that the model is not capable of generalization. We have demonstrated that the main cause of the problem is inherent in the drone arena, thus we want to remove the room from the equation. We propose a solution that consists of performing advanced data augmentation by just keeping the person in the images, masking out the background to be randomly replaced with something else.

This section explores various algorithms and experiments for creating the mask of a person in an image, that ended with the final adoption of Mask R-CNN.



\subsection{Canny}
\label{subsec:masking-canny}

The first experiments are based on a classic computer vision technique called Canny Edge Detection. A custom algorithm\footnote{adapted from \url{https://stackoverflow.com/a/29314286/10866825}} applies the related function from OpenCV\cite{opencv_canny} to find the edges inside the image, which are then used for also finding the contours. Only the biggest contour is taken into consideration for building a mask around the subject in the image.

\medskip

A core aspect of the Canny function for finding appropriate contours is the choice of its parameters \texttt{minVal} and \texttt{maxVal}. These are used by the algorithm for distinguishing between \textit{sure-edges}, \textit{probable-edges} and \textit{no-edges}. Several experiments have been made with different values, but no combination of the two parameters is optimal on our dataset.

Figure \ref{fig:canny-overview} shows what happens with \texttt{minVal} and \texttt{maxVal} respectively set to 100 and 400. Most of the time the person is well detected, while other times it completely disappears or even results in a fatal error (red frames). The room baseboard (the line between the floor and the wall) is often still present in the image, while many samples seem to preserve a huge portion of the background.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=1\textwidth]{"contents/images/04-1canny-overview"}
	\caption[Canny edge detection overview on the training set]{Canny edge detection overview on the training set}
	\label{fig:canny-overview}
\end{figure}

\medskip

In some cases, it even happens that the body of the person is present in the image while the face disappears. For mitigating this problem, an enhanced version of the algorithm has been considered, designed to always keep the person's face and part of the body in the resulting image, assuming their positions are known. Figure \ref{fig:canny-enanhced} illustrates the problem and demonstrates that results obtained from the enhanced version are still not acceptable, since we are not able enough to appropriately remove the background from the scene.

\begin{figure}[!h]
	\begin{center}
		\begin{subfigure}[h]{1\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-1canny-enhance-1"}
		\end{subfigure}
		\vfill
		\begin{subfigure}[h]{1\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-1canny-enhance-2"}
		\end{subfigure}
		\vfill
		\begin{subfigure}[h]{1\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-1canny-enhance-3"}
		\end{subfigure}
	\end{center}
	\vspace{-0.5cm}
	\caption[Canny edge enhanced algorithm demonstration]{Canny edge enhanced algorithm demonstration}
	\label{fig:canny-enanhced}
\end{figure}



\subsection{GrabCut}
\label{subsec:masking-grabcut}

GrabCut (\cite{opencv_grabcut}) operates by using the subject position in the image and some statistical inference for labeling each pixel of the image as background or foreground. It works as follows:

\begin{itemize}
	\item User inputs the rectangle. Everything outside this rectangle will be taken as sure background. Everything inside the rectangle is unknown. Similarly, any user input specifying foreground and background are considered as hard-labeling which won't change in the process.
	\item Computer does initial labeling depending on the given data. It labels the foreground and background pixels (or it hard-labels).
	\item From now on, a \gls{gmm} is used to model the foreground and background.
	\item Depending on the data given to GrabCut, \gls{gmm} learns and creates a new pixel distribution. That is, the unknown pixels are labeled either \textit{probable-foreground} or \textit{probable-background} depending on their relationship with other hard-labeled pixels in terms of color statistics (like clustering).
	\item A graph is built from this pixel distribution. Nodes in the graphs are pixels. Additional two nodes are added, the Source node and the Sink node. Every foreground pixel is connected to the Source node and every background pixel is connected to the Sink node.
	\item The weights of edges connecting pixels to source or end nodes are defined by the probability of a pixel being foreground or background. The weights between the pixels are defined by the edge information or pixel similarity. If there is a large difference in pixel color, the edge between them will get a low weight.
	\item Then a min-cut algorithm is used to segment the graph. It cuts the graph into two, separating the source node and the sink node with a minimum cost function. The cost function is the sum of all weights of the edges that are cut. After the cut, all the pixels connected to the Source node become foreground and those connected to the Sink node become background.
	\item The process is continued until the classification converges.
\end{itemize}


OpenCV GrabCut function\footnote{\url{https://docs.opencv.org/4.1.2/d7/d1b/group__imgproc__misc.html\#ga909c1dda50efcbeaa3ce126be862b37f}} has two initialization modalities. You can only pass the rectangle, as described in the algorithm, or a mask of the image in which you specify whether a certain pixel is \textit{sure-background}, \textit{probable-background}, \textit{probable-foreground} or \textit{sure-foreground}. These classes\footnote{\url{https://docs.opencv.org/master/d7/d1b/group__imgproc__misc.html\#gad43d3e4208d3cf025d8304156b02ba38}} are also used by the library during the algorithm itself. 

\medskip

Both approaches require previous knowledge about the subject position in the image. For now, let’s manually provide this information for the example images. Later on, we will consider an algorithm for automatic human detection.


\subsubsection{Rectangle initialization}
\label{subsec:masking-grabcut-rect}

This approach requires that a rectangle, entirely containing the subject, is given in input to the function (figure \ref{fig:grabcut-rect-explain-1}). GrabCut proceeds as follows. Area inside the rectangle is marked as \textit{probable-background} (green), while the pixels outside are \textit{sure-background} (blue). As the algorithm keeps going, it finds pixels inside the rectangle which can be foreground, marking them as \textit{probable-foreground} (yellow) (figure \ref{fig:grabcut-rect-explain-2}). Later, we binarize and smooth the mask (figure \ref{fig:grabcut-rect-explain-3}) for finally removing the background from the original image.

\begin{figure}[!h]
	\begin{center}
		\begin{subfigure}[h]{0.32\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-2grabcut-1rect-steps-1"}
			\caption[]{Initialization}
			\label{fig:grabcut-rect-explain-1}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.32\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-2grabcut-1rect-steps-2"}
			\caption[]{Pixel classes}
			\label{fig:grabcut-rect-explain-2}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.32\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-2grabcut-1rect-steps-3"}
			\caption[]{Final mask}
			\label{fig:grabcut-rect-explain-3}
		\end{subfigure}
	\end{center}
	\vspace{-0.5cm}
	\caption[Grabcut algorithm explained: rectangle initialization]{Grabcut algorithm explained: rectangle initialization}
	\label{fig:grabcut-rect-explain}
\end{figure}

Performance obtained by the algorithm is available in figure \ref{fig:grabcut-rect-example}. Results seem better than the ones produced by Canny Edge Detection. However, it happens that the face or the entire person is filtered out of the image.

\begin{figure}[!h]
	\begin{center}
		\begin{subfigure}[h]{1\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-2grabcut-1rect-example-1"}
		\end{subfigure}
		\vfill
		\begin{subfigure}[h]{1\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-2grabcut-1rect-example-2"}
		\end{subfigure}
	\end{center}
	\vspace{-0.5cm}
	\caption[Grabcut demonstration: rectangle initialization]{Grabcut demonstration: rectangle initialization}
	\label{fig:grabcut-rect-example}
\end{figure}


\subsubsection{Mask initialization}
\label{subsec:masking-grabcut-mask}

In order to improve the previous technique, we try mask initialization, which allows us to initially classify the pixels in the image as we prefer. This time, we do consider the pose information for telling GrabCut we already know that some part of the image is \textit{sure-foreground}: face and part of the body.

Let's now consider an example of an image for which the previous solution was completely missing the person in the result despite the well-initialized rectangle.
In figure \ref{fig:grabcut-mask-explain}, we notice manually assigned \textit{sure-foreground} pixels in blue, while GrabCut inferred \textit{probable-foreground} in yellow and \textit{probable-background} in green. Results are undoubtedly better, but we notice that the left-most background has been kept in the final image, while we could easily identify it as \textit{sure-background} using the person pose we assume as known, like in the previous rectangle initialization.

\begin{figure}[!h]
	\begin{center}
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-2grabcut-2mask-steps-1"}
			\caption[]{Initialization}
			\label{fig:grabcut-mask-explain-1}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-2grabcut-2mask-steps-2"}
			\caption[]{Pixel classes}
			\label{fig:grabcut-mask-explain-2}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-2grabcut-2mask-steps-3"}
			\caption[]{Final mask}
			\label{fig:grabcut-mask-explain-3}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-2grabcut-2mask-steps-4"}
			\caption[]{Result}
			\label{fig:grabcut-mask-explain-4}
		\end{subfigure}
		\hfill
	\end{center}
	\vspace{-0.5cm}
	\caption[Grabcut algorithm explained: mask initialization]{Grabcut algorithm explained: mask initialization}
	\label{fig:grabcut-mask-explain}
\end{figure}


\subsubsection{Hybrid initialization}
\label{subsec:masking-grabcut-hybrid}

We finally try a mixed approach between rectangle and mask initializations, by specifying both the person and face positions in the image. This allows us to initially set sure-background, probable-background, and sure-foreground pixels. Only probable-foreground pixels have to be found by the GrabCut algorithm. 

In figure \ref{fig:grabcut-hybrid-explain} the usual explanation. Here, \textit{sure-background} is dark blue, \textit{probable-background} is green, \textit{probable-foreground} is yellow and \textit{sure-foreground} is light blue. 
Image \ref{fig:grabcut-hybrid-example} shows results of hybrid initialization applied to the same samples introduced in the figure \ref{fig:grabcut-rect-example}. Results are sub-optimal, with excellent segmentation.

\begin{figure}[!h]
	\begin{center}
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-2grabcut-3hybrid-steps-1"}
			\caption[]{Initialization}
			\label{fig:grabcut-hybrid-explain-1}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-2grabcut-3hybrid-steps-2"}
			\caption[]{Pixel classes}
			\label{fig:grabcut-hybrid-explain-2}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-2grabcut-3hybrid-steps-3"}
			\caption[]{Final mask}
			\label{fig:grabcut-hybrid-explain-3}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-2grabcut-3hybrid-steps-4"}
			\caption[]{Result}
			\label{fig:grabcut-hybrid-explain-4}
		\end{subfigure}
		\hfill
	\end{center}
	\vspace{-0.5cm}
	\caption[Grabcut algorithm explained: hybrid initialization]{Grabcut algorithm explained: hybrid initialization}
	\label{fig:grabcut-hybrid-explain}
\end{figure}

\begin{figure}[!h]
	\centering
	\includegraphics[width=1\textwidth]{"contents/images/04-2grabcut-3hybrid-final"}
	\caption[Grabcut demonstration: hybrid initialization]{Grabcut demonstration: hybrid initialization}
	\label{fig:grabcut-hybrid-example}
\end{figure}


\subsubsection{Automatic Human Detection}
\label{subsec:masking-yolo}

Now that we have a promising background removal algorithm, to let it work we need to infer person and face position from the image. \gls{gt} data is not sufficiently precise for providing such information, so we try other state-of-the-art object detection techniques. For properly using hybrid initialization, that we have demonstrated to be the most precise solution, we need two pieces of information: the bounding boxes associated with both the entire person and its head. For this reason, distinct detectors are necessary.

\medskip

We adopt YOLO (\cite{redmon2016look}) for human detection, through the \texttt{cvlib}\footnote{\url{https://www.cvlib.net/}} library that implements a YOLOv3 model trained on the Microsoft COCO dataset (\cite{lin2015microsoft}), capable of detecting 80 common objects in context. Underneath, it uses the OpenCV dnn module\footnote{\url{https://docs.opencv.org/master/d2/d58/tutorial_table_of_content_dnn.html}}.

A demo on our dataset is shown in figure \ref{fig:yolo}, where we notice that YOLO overall provides quite good results. However, in 10-20\% of the cases, it does not detect any object in the image, most probably because of their low resolution.

\begin{figure}[!h]
	\centering
	\includegraphics[width=1\textwidth]{"contents/images/04-3yolo"}
	\caption[YOLO demonstration, which shows failures for 2 images]{YOLO demonstration, which shows failures for 2 images}
	\label{fig:yolo}
\end{figure}

\medskip

For enabling mask initialization, also the face position is needed. We firstly try to heuristically infer its position based on the bounding box of the entire person provided by YOLO. The majority of samples are compliant with this heuristic, but a non-ignorable percentage of samples is not compatible. For this reason, we also try an open-source head detector\footnote{\url{https://github.com/AVAuco/ssd_head_keras}}, miserably failing in its task due to the small size of our images.

\bigskip

While searching for a solution compatible with such low-fidelity images, we find an all-in-one solution, presented in the next section, that immediately became our choice for its surprising results.



\clearpage
\subsection{Mask R-CNN}
\label{subsec:masking-maskrcnn}

Mask R-CNN (\cite{he2018mask}) is a state of the art deep learning framework for object detection and instance segmentation, whose technical details have been illustrated in section \ref{subsec:sota-maskrcnn}. Originally developed by Facebook researchers in \cite{pytorch}, now available in the \texttt{Detectron2} package (\cite{wu2019detectron2}), the algorithm has been ported to TensorFlow 1 (\cite{MaskRCNN_matterport}) and later adapted for TensorFlow 2 by \cite{MaskRCNN_akTwelve}\footnote{\url{https://github.com/akTwelve/Mask_RCNN}}. The latter has been used for applying Mask R-CNN on our dataset images.

\medskip

Results are incredibly precise and the method undoubtedly outperforms any other previously experimented, as it is able to provide both human detection and segmentation at once. Figure \ref{fig:maskrcnn-dario} below presents how Mask R-CNN easily detects people in our video frames, regardless of their low resolution and any light condition or person position. In many cases, but not always, multiple people or objects in the background are correctly detected, even if they are very small.

\begin{figure}[!h]
	\begin{center}
		\begin{subfigure}[h]{0.32\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-maskrcnn-train-1"}
		\end{subfigure}
		\begin{subfigure}[h]{0.32\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-maskrcnn-train-2"}
		\end{subfigure}
		\begin{subfigure}[h]{0.32\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-maskrcnn-train-3"}
		\end{subfigure}
		\vfill
		\begin{subfigure}[h]{0.32\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-maskrcnn-train-4"}
		\end{subfigure}
		\begin{subfigure}[h]{0.32\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-maskrcnn-train-5"}
		\end{subfigure}
		\begin{subfigure}[h]{0.32\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-maskrcnn-train-6"}
		\end{subfigure}
	\end{center}
	\vspace{-0.5cm}
	\caption[Mask R-CNN applied to our training set]{Mask R-CNN applied to our training set}
	\label{fig:maskrcnn-dario}
\end{figure}

This high-level of accuracy in detection and segmentation comes with an extremely high computing power requirement\footnote{accordingly to the original paper, Mask R-CNN can only run at 5 \gls{fps}}. For reference, running Mask R-CNN on the test set - composed of about 11'000 images - requires a total computing time\footnote{we observe, using the \texttt{time} command for IPython, the following CPU times: user 35min, sys 20min, total 55min; and Wall time: 1h 4min} of approximately 55 minutes on Google Colab using a GPU runtime\footnote{equipped with NVIDIA T4 GPU}.

Because of this, the inference on the images must be done offline. The training procedure will only receive, together with each input image, the previously computed user's mask. This will be used to perform background replacement.









