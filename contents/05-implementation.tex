\chapter{Model Implementation}
\label{chap:implementation}

In this chapter, we explain how the proposed solution has been implemented.

Firstly, we focus on the implementation of our generalization strategy, mainly concerning the way the dataset is treated and processed to reduce overfitting. Then, we provide technical details about the training procedure, with a particular focus on time performance. Section \ref{subsec:model-variants} provides three model alternatives, that will be considered for evaluation and comparison in chapter \ref{chap:evaluation}.




\section{Background Replacement}
\label{sec:implementation-bgreplace}

As demonstrated in section \ref{subsec:gradcam-results}, the approach defined by \cite{mantegazza2019visionbased} is lacking generalization capabilities. The main reason behind this problem is attributable to its dataset composition. More specifically, the model is biased by many elements appearing in the drone arena, in which the data have been originally collected. As a solution to eliminate the problem, we modify the training set by performing background replacement on its images.

In section \ref{subsec:masking-maskrcnn}, we anticipated the use of Mask R-CNN to pre-process the dataset. The algorithm detects and creates a mask for all the objects appearing in the input images, labeling each mask with the category to which the object belong (e.g., person, TV, bike, car, etc.). However, for our purposes, we are only interested in the mask corresponding to the user who is actually facing the drone. Since the user is always the nearest person to the drone's camera, its mask must be the one with the largest size among all people's masks found by Mask R-CNN.

To perform the background replacement, the training procedure receives together with each sample also the corresponding user's mask. This is used for distinguishing the subject from the rest of the image, to accordingly blend the camera's frame with another image, serving as the background. The ground truth remains unchanged, and this is the main advantage of our approach. We can simulate a dataset acquired in different environments without the need of actually collecting it, which would otherwise require a dedicated \gls{mocap} system. Our method is fairly similar to domain randomization (section \ref{subsec:domain-randomization}), a technique widely applied in robotics to train \gls{ml} models in simulated virtual environments.

By providing a considerable amount of images to use as backgrounds, the \gls{ml} model trained on the modified dataset should be able to actually ignore the background. The \gls{cnn}, instead, will hopefully learn the concept of a person, in order to predict its position in any condition.

\medskip

For our work, we select the publicly available dataset\footnote{\url{http://web.mit.edu/torralba/www/indoor.html}} for Indoor Scene Recognition presented during the 9th Conference on Computer Vision and Pattern Recognition (CVPR). Created by researchers from MIT (\cite{cvpr09}), the dataset contains a total of 15'620 images divided into 67 indoor categories. For our task, categorization is not actually needed, but it ensures a good variety of scenarios to present to the model. For shortness, in this thesis, the dataset will be referred to as \textit{CVPR}.

During training, each sample is assigned to a randomly chosen background from the CVPR dataset. Figure \ref{fig:bgreplace-example} shows a demonstration applied to the samples previously presented in figure \ref{fig:frontalnet-dataset-overview}. 

\vspace*{5ex}
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.98\textwidth]{"contents/images/05-bgreplace-overview"}
	\caption[Example of background replacement on the training set]{Example of background replacement on the training set}
	\label{fig:bgreplace-example}
\end{figure}
\clearpage




\section{Image Augmentation}
\label{sec:implementation-imgaug}

Data augmentation is a common regularization technique for improving the ability of neural networks to generalize their task on unknown data. Introduced in section \ref{subsec:data-augmentation}, image augmentation is ubiquitously used with \gls{cnn}s for reducing overfitting on the training images by applying random transformations.

Our implementation relies on Albumentations (\cite{Buslaev_2020}), a state of the art Python library which provides a huge variety of image augmentations. Constantly updated and well-documented, Albumentations can boast the best benchmarking performance in the field\footnote{\url{https://github.com/albumentations-team/albumentations\#benchmarking-results}}.

\medskip

Our \gls{cnn} learns to predict the user's pose, but the relation between the person's position in the image and the ground is not known a priori. Being not able to modify the ground truth according to affine transformations, spatial-level augmentation (see section \ref{subsec:data-augmentation}) is not an option in our case. Instead, we only apply pixel-level augmentations. 

The only exception is for horizontal flipping, which only requires inverting the \texttt{Y} coordinate in the ground truth. As explained in section \ref{subsec:dataset-composition}, the \texttt{Y} coordinate represents the horizontal alignment of the user \gls{wrt} the drone. Figure \ref{fig:frontalnet-dataset-distribution-class}, in the previous chapter, clearly shows a propensity for the user to be on the right-part of the images. For this reason, we decide to mirror the camera's frames horizontally with a probability of 50\%.

\medskip

According to user-defined probabilities, Albumentations gives the opportunity to apply a set of different augmentations with variable intensities. Among pixel-level transformations, the possibilities are limitless and they can produce aggressively augmented images, which might be very different from the originals. Figure \ref{fig:albumentation-example} provides a good example of augmentation, applied both on original and background-augmented samples. Those results are obtained combining transformations on brightness and contrast, multiplicative noise, channels manipulation, and rectangular dropouts.

\begin{figure}[!h]
	\begin{center}
		\begin{subfigure}[h]{0.49\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/05-imgaug-example-1"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.49\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/05-imgaug-example-2"}
		\end{subfigure}
	\end{center}
	\vspace{-0.5cm}
	\caption[Example of image augmentation with Albumentations]{Example of image augmentation with Albumentations}
	\label{fig:albumentation-example}
\end{figure}

The combination of different transformations composes a pipeline, whose time performance depends on custom choices and probabilities. Among all tested\footnote{tests performed on a notebook equipped with an Intel Core i7-6700HQ CPU @ 2.60 GHz} augmentations, most of them only require a maximum of 0.7 seconds to be applied on a set of 10'000 $60 \times 108$ images, while the most expensive ones take up to 12 seconds under the same conditions\footnote{Benchmarks (in seconds): InvertImg 0.2; ToGray 0.2; ChannelShuffle 0.3; ChannelDropout 0.4; Blur 0.5; RandomGamma 0.5; RandomBrightnessContrast 0.6; Solarize 0.6; Equalize 0.7; MotionBlur: 0.7; HueSaturation 1.5 sec; RGBShift 1.5 sec; CLAHE 3.5 sec; CoarseDropout 3.5 sec; MultiplicativeNoise 7 sec; GaussNoise 10 sec; ISONoise 12 sec}.

\medskip

The Albumentations pipeline adopted for our work is shown in listing \ref{lst:albumentations}. It takes about 4 seconds to process 10'000 images and accepts a parameter \texttt{aug\_prob} to define the prior probability of actually applying the augmentations to an input image. Some examples of resulting images are available in figure \ref{fig:albumentation-chosen}.

\vspace{0.1cm}
\begin{python}
augmenter = A.Compose([
	A.RandomBrightnessContrast(brightness_by_max=True, p=0.75),
	A.RandomGamma(p=0.5),
	A.CLAHE(p=0.05),
	A.Solarize(threshold=(200, 250), p=0.2),
	A.OneOf([
		A.Equalize(by_channels=False, p=0.5),
		A.Equalize(by_channels=True, p=0.5),
	], p=0.1),
	A.RGBShift(p=0.3),
	A.OneOf([
		A.ChannelDropout(fill_value=128, p=0.2),
		A.ChannelShuffle(p=0.8),
	], p=0.1),
	A.MultiplicativeNoise(per_channel, elementwise, p=0.05),
	A.CoarseDropout(holes=(20, 70), size=(1, 4), p=0.2),
	A.ToGray(p=0.05),
	A.InvertImg(p=0.05),
	A.OneOf([
		A.Blur(blur_limit=4, p=0.5),
		A.MotionBlur(blur_limit=6, p=0.5),
	], p=0.05),
], p=aug_prob)
\end{python}
\vspace{-0.5cm}
\begin{lstlisting}[frame=none,caption={Chosen Albumentations pipeline}, 
label=lst:albumentations]
\end{lstlisting}

At the end of the pipeline, we also apply Perlin noise (\cite{perlin-noise}) with a probability of 20\%. Injecting noise into images can greatly help \gls{cnn}s on avoiding overfitting (\cite{shorten2019augmentationsurvey}). Since Perlin noise generation is highly time-consuming, a set of textures has been pre-computed. During training, for each augmented sample, one of the generated noises is randomly chosen, cropped, flipped, and finally applied to the input image. Half of the time, Perlin noise is multiplied uniformly on all channels to produce a grayscale texture, while the other half is differentiated over RGB channels. Picture \ref{fig:perlin-noise} illustrates both cases.

\begin{figure}[!h]
	\centering
	\includegraphics[width=1 \textwidth]{"contents/images/05-imgaug-chosen"}
	\caption[Examples of the chosen image augmentation pipeline]{Examples of the chosen image augmentation pipeline}
	\label{fig:albumentation-chosen}
\end{figure}

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.75\textwidth]{"contents/images/05-imgaug-pelin"}
	\caption[Perlin noise example]{Perlin noise example. From left to right: (1) randomly chosen, cropped and flipped 3-channels texture (2) applied uniformly (3) applied by channel}
	\label{fig:perlin-noise}
\end{figure}



\section{Model Variants} % define the models
\label{sec:model-variants}

\section{Data Generator}
\label{sec:data-generator}

- posso spiegare il generator generico e poi parlare di ottimizzazione oppure farlo tutto insieme


\section{Training}
\label{sec:implementation-training}

Our first experiments (INSERIRE QUALI) were carried out with Jupyter Notebooks via Google Colab on a GPU-accelerated runtime, while the final code is provided as Python scripts. 

ESPANDERE QUESTI DETTAGLI
For debugging, we use a Windows 10 laptop equipped with an NVIDIA GeForce GTX 950M graphic card, while training is performed on a dedicated Ubuntu 18.04 workstation available at \gls{idsia}, mounting four NVIDIA GeForce RTX 2080 Ti \footnote{anyway, multiple available GPUs are used for single-GPU computing, not simultaneously}.

- inserisco dettagli hardware
    2x Intel Xeon Gold 5217 8-core 3GHz (32 thread)
    128 GB RAM
    
- quanto ci ha messo a trainare etc



