\chapter{Theoretical Foundation}
\label{chap:theory}

%\lipsum[1]



\section{Robotics}
\label{sec:robotics}

%\subsection{Pose}
%\label{subsec:robot-pose}
%
%\subsection{Control Theory}
%\label{subsec:robot-control}




\section{Machine Learning}
\label{sec:machine-learning}

%\subsection{Supervised Learning}
%\label{subsec:supervised-learning}
%
%\subsection{Convolutional Neural Networks}
%\label{subsec:cnn}
%
%\subsection{Residual Neural Networks}
%\label{subsec:resnet}




\section{Human-drone Interaction}
\label{sec:human-drone-interaction}

A good variety of research can be found on human-robot interaction and a lot is yet to come. In the field, drones represent a specific segment due to their ability to freely move in the 3D space, opening access to new use cases while representing a real challenge for professionals and researchers.

In this section, we firstly present a general overview on the topic, then we focus on related works by \gls{idsia}.



\subsection{The State of the Art of Human–Drone Interaction}
\label{subsec:human-drone-sota}

Drones attracted a lot of attention in the last years and their usage is expected to keep growing, thanks to decreasing costs and powerful features they can provide both for personal, commercial, and social usage.

An article published in Nov 2019 for the IEEE Access \cite{human-drone-sota} explores literature and state of art for human-drone interaction. It reports that \gls{faa} expects total drone registrations to increase by more than 60\% between 2018 and 2022. The main increment will be seen in the commercial sector, rather than the hobbyist one, even though the latter still counts the the large majority of units. Nowadays, almost half of drone usage is for aerial photography (48\%), followed by industrial inspection (28\%) and agriculture (17\%). The authors argue that in the next decade drones will become ubiquitous to society, and extensively used in advertising, shipping, sports, emergency, and many other fields for augmenting human capabilities.

Providing a nice overview of the latest works, the article highlights the most important challenges of today's research in human-drone interaction. The main studies in the area treat drones' control and human-machine communication, while the principal concerns regard the users' perception of safety during flight.

These topics are strictly related to our task, concerning autonomous flight in human proximity.



\subsection{Vision-based Control of a Quadrotor in User Proximity}
\label{subsec:sota-dario}

Our work is built upon the original master thesis \cite{mantegazza2018thesis} and paper \cite{mantegazza2019visionbased} from Dario Mantegazza, named \textit{Vision-based Control of a Quadrotor in User Proximity: Mediated vs End-to-End Learning Approaches}, developed at \gls{idsia} between 2018 and 2019.

\medskip

In his thesis, the author proposes a machine learning technique for teaching a model to control a drone interacting with a person. In a few words, the goal is to continuously fly the drone to hover in front of a moving user. The problem is approached as a reactive control procedure and addressed with supervised learning. Thus, it provides an interesting starting point for many other robotics applications.

Training data is acquired by programmatically flying the drone to frontally face a user. In this phase, the quadrotor is controlled through an omniscient controller that knows both the drone's and person's pose. Images produced by the front-facing camera of the drone are used as input for a custom-designed \gls{resnet} architecture, that infers the relative user's position \gls{wrt} the drone. Practically, the neural network performs a regression on the four variables that form the user's pose (X, Y, Z, YAW), and learns to predict their values by using spatial information contained in the input images.

In the paper, the author also makes a comparison between the mediated approach described above and another end-to-end approach based on imitation learning. This second solution directly learns control signals\footnote{desired pitch, roll, yaw, and vertical velocity} for the drone, instead of the user's pose. A third experiment also considers the task of automatically learning a controller which drives the drone based on the given user's position.

All the solutions provide very similar results, but for this thesis we will only take into consideration the mediated approach. The reason behind this choice is that the learned model can be adapted to other tasks by simply designing a custom controller for the drone. It also provides a more transparent and analyzable solution.

\medskip

Even though this kind of problems on human recognition and pose estimation could be faced with more advanced deep learning algorithms, making a simple regression on four variables allows the network to be fairly small, so that the prediction task is light, fast to execute, and possibly portable on low-end devices.

Our entire project makes use of the original network architecture and dataset defined in {mantegazza2019visionbased}, respectively explored in sections \ref{subsec:frontalnet-architecture} and \ref{sec:dataset}, For a better understanding, the original code repository\footnote{\url{https://github.com/idsia-robotics/proximity-quadrotor-learning}} and a descriptive video\footnote{\url{https://drive.switch.ch/index.php/s/MlEDrsuHcSl5Aw5}} are also available. 
To enhance readability, having no official name, the custom \gls{resnet} architecture proposed by the author will be simply called \textit{FrontalNet}.


\subsubsection{FrontalNet Architecture}
\label{subsec:frontalnet-architecture}

The network is composed of a total of 1'332'484 trainable parameters, accepts a single $60 \times 108$ pixels image in input, and outputs 4 regression variables that correspond to the user's pose coordinates. Each \gls{resnet} block is provided with batch normalization, ReLU activations \cite{act-relu} are used for all layers except for the output neurons, which are associated with a linear activation function \cite{act-linear}. 

Figure \ref{fig:frontalnet-architecture-1} provides an illustration of the mediated architecture we consider for this thesis\footnote{take a look at image \ref{fig:frontalnet-architecture-paper3approaches} for details about the architectures related to the other approaches}. A complete list of all layers is also available in figures \ref{fig:frontalnet-architecture-3a} and \ref{fig:frontalnet-architecture-3a} of the appendix. 

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.3\textwidth]{"contents/images/03-frontalnet-1A"}
	\caption[Schematic FrontalNet architecture]{Schematic FrontalNet architecture \cite{mantegazza2019visionbased}}
	\label{fig:frontalnet-architecture-1}
\end{figure}


\subsubsection{FrontalNet Performance}
\label{subsec:frontalnet-performance}

FrontalNet is trained using the \gls{mae} loss function with the \gls{adam} optimizer \cite{kingma2014adam} and a base learning rate of 0.001, progressively reduced on validation loss plateaus that last more than 5 epochs. A maximum of 200 epochs are run in total, but with an early stopping policy with patience of 10 epochs on the validation loss. 

Performance is evaluated both quantitatively and qualitatively on the end-to-end model, rather than the mediated one considered for our work. However, as explained before, both approaches obtain similar results. We can accordingly consider the following evaluation to be valid for the mediated approach too.

\medskip 

\glsreset{r2}
For quantitative evaluation, the chosen metric is \gls{r2}\footnote{\gls{r2} interpretation will be explained in the evaluation chapter \ref{chap:evaluation}}, which has an interval of $[-\inf, 1]$, where 1 represents the optimality. 

The author also conducts an experiment for finding the minimum cardinality of the dataset to obtain acceptable performance. Results are available in figure \ref{fig:frontalnet-r2}, directly taken from the paper. As shown, at least 5,000 samples are required to achieve decent performance, which continues to improve as the size of the training set increases.

Specifically, predictions seem more accurate for variables \texttt{Z} and \texttt{W} with an \gls{r2} score of 0.82 and 0.88, respectively. Quite different the results for \texttt{X} and \texttt{Y} which only reach an \gls{r2} of 0.59 and 0.57, respectively.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.8\textwidth]{"contents/images/03-frontalnet-r2"}
	\caption[FrontalNet \gls{r2} results \cite{mantegazza2019visionbased}]{FrontalNet \gls{r2} results \cite{mantegazza2019visionbased}. A1, A2 and A3 in the chart stands for different models, but they anyway achieve almost the same results.}
	\label{fig:frontalnet-r2}
\end{figure}

\medskip

The previous considerations on the variables are confirmed by the qualitative evaluation, obtained by comparing ground truth and predictions during a short simulation. Figure \ref{fig:frontalnet-gt-pred} shows that X and Y predictions are considerably worse than results achieved by Z and W when compared with the ground truth.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.8\textwidth]{"contents/images/03-frontalnet-gt-pred-2"}
	\caption[FrontalNet GT vs prediction results \cite{mantegazza2019visionbased}]{FrontalNet GT vs prediction results \cite{mantegazza2019visionbased}. A1, A2 and A3 in the chart stands for different models, but they anyway achieve almost the same results.}
	\label{fig:frontalnet-gt-pred}
\end{figure}


\subsection{Embedded Implementation of Controller for Nano-Drones}
\label{subsec:sota-nicky}

Autonomous navigation is an important and well-known area of research in robotics, which usually requires accomplishing complex and computationally-expensive tasks such as localization, mapping and path planning. Recent studies have started to approach autonomous driving through imitation learning \cite{imitation_learning_survey}, where neural networks learn by imitating human behavior in specific tasks. 

\medskip

In 2018, researchers at the UZH University of Zürich have demonstrated that \gls{resnet}s are able to provide satisfactory performance in the field. They developed DroNet \cite{Loquercio_2018}, a forked \gls{cnn} that predicts, from a single gray-scale image, a steering angle and a collision probability. In other words, the model learns to steer and avoid obstacles from forward-looking videos recorded by cars and bikes while driving in real contexts. In this case, both the prediction and controller tasks were powered off-drone on a dedicated computer, remotely connected through WiFi. 

A year later, ETH Zürich was able to develop PULP-DroNet \cite{palossi2019pulpdronetIoTJ}, porting the \gls{cnn} on the Crazyflie\footnote{\url{https://www.bitcraze.io/products/crazyflie-2-1/}}, a nano-drone with a size of only 3 $3 \times 3$ centimeters for a weight of 27 grams. They propose a general methodology for deploying on-board deep learning algorithms for ultra-low-power devices, without any need for an external laptop to run the software.

\medskip

Inspired by PULP-DroNet, \gls{idsia} adapted its FrontalNet to work on-board the Crazyflie with excellent results \cite{zimmerman2020thesis}. The nano-drone is able to achieve good quantitative and qualitative performance, regardless of any problem deriving from working with such low-end devices. The main challenges are represented by low computational power, energy consumption management, and low-fidelity camera with no video stabilization\footnote{Himax HM01B0 camera, theoretically able to produce $320 \times 320$ \gls{mp} images at 60 FPS. However, the frame rate is incredibly reduced during data collection due to some platform limitation in image transferring.}.




\section{Network Interpretability}
\label{sec:network-interpretability}

\glsreset{ml}
\glsreset{nn}

A \gls{nn} learns abstract representations for finding a mapping between its input and output, determined by well-defined mathematical computations that involve the input itself and the progressively learned network parameters. Inspired by biological brains, this approach seems to be incredibly effective on a huge variety of tasks.

\medskip 

However, unlike other \gls{ml} techniques, \gls{nn} are known to produce "black-box" models. Their reasoning and comprehension are intrinsic in the network parameters, which are nothing but numbers, particularly hard to understand even from domain experts. 

When working with real-world problems, it is extremely important to be able to explain what a \gls{ml} model is actually understanding. This builds trust in algorithms and makes sure there are no undesirable biases in the models, which could raise serious problems, especially in critical fields such as medicine and law.

\medskip

\gls{xai} is the field of study which tries to make \gls{ml} results, and their underlying basis for decision-making, properly understandable to humans \cite{xai-wiki}. For \gls{cnn}s, researchers have developed many techniques for understanding what a \gls{nn} actually care of when producing an output based on an input image. 

Main efforts regard feature visualization and attribution, but recent advanced studies have also shown how these methods can be used altogether \cite{olah2018the}. This section briefly explains these two major areas for \gls{cnn} interpretability, with a particular focus on spatial attribution, the chosen methodology for our work.



\subsection{Feature Visualization}

\textbf{TODO}

Sources: \cite{olah2017feature}



\subsection{Spatial Attribution with GradCAM}
\label{subsec:gradcam-theory}

\textbf{TODO}

Sources: \cite{Selvaraju_2019}, \cite{gradcam_medium}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.9\textwidth]{"contents/images/gradcam/02-gradcam-schema"}
	\caption[\gls{gradcam} schematic functioning]{\gls{gradcam} schematic functioning \cite{Selvaraju_2019}}
	\label{fig:gradcam-schema}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.9\textwidth]{"contents/images/gradcam/02-gradcam-catdog"}
\caption[\gls{gradcam} example on dog-cat classification]{\gls{gradcam} example on dog-cat classification \cite{Selvaraju_2019}}
\label{fig:gradcam-catdog}
\end{figure}




\section{Network Generalization}
\label{sec:network-generalization}

\textbf{TODO}



\subsection{Data Augmentation}
\label{subsec:data-augmentation}

\textbf{TODO}

Random Erasing Data Augmentation: \cite{zhong2017random}

Previous works on images:  \cite{yue2019domain}, \cite{Takahashi_2020}, \cite{xie2020unsupervised}

AutoAugment: \cite{cubuk2019autoaugment}
Learning Data Augmentation Strategies for Object Detection: \cite{zoph2019learning}

\medskip

Image transformations can be divided into two types:
\begin{itemize}
	\item Spatial-level augmentations: also called affine transformations, they relocates pixels by cropping, scaling, rotating, translating, mirroring and shearing the images. These transformations also have to accordingly modify additional elements associated with the images such as ground truth, masks, bounding boxes or keypoints.
	\item Pixel-level augmentations: algorithms which only modify pixels (or group of pixels) without relocating them or changing the image shape. These transformations leave unchanged any other additional element associated with the images such as ground truth, masks, bounding boxes or keypoints.
\end{itemize}


\subsection{Domain Randomization}
\label{subsec:domain-randomization}

\textbf{TODO}

Summary from \cite{mehta2019active}

See \cite{weng2019DR_explanation}, \cite{tobin2017domain}
 
"Domain randomization is a popular technique for improving domain transfer, often used in a zero-shot setting when the target domain is unknown or cannot easily be used for training. In this work, we empirically examine the effects of domain randomization on agent generalization. Our experiments show that domain randomization may lead to suboptimal, high-variance policies, which we attribute to the uniform sampling of environment parameters."

\medskip

Imitation learning through simulation is recently becoming an interesting and successful approach for both reinforcement learning \cite{imitation_learning_3d_navigation} and object recognition \cite{tobin2017domain} \cite{weng2019DR}.

Robot and environment can be replicated through a dedicated simulator such as Gazebo\footnote{\url{http://gazebosim.org/}}, often used in robotics with \gls{ros}\footnote{\url{https://www.ros.org/}} due to its straightforward integration. Even general-purpose graphic engines can be used, which includes Unreal Engine\footnote{\url{https://www.unrealengine.com/en-US/}} Unity\footnote{\url{https://unity.com/}} are well-known simulators designed for game-development, but recently used for \gls{vr} and \gls{ar} applications. They give developers unlimited possibilities, carefully supported by solid physics engines and active communities.

\medskip

Given the difficulty of collecting data for our task, exploring the possibility of simulating the entire scenario in a 3D virtual-world is intriguing, especially to replace the need for a complex \gls{mocap} system. Integrating odometry support, drone and people can be thoroughly modeled to act as in the real world, with similar movements and sensing capabilities, in order to collect the data very efficiently. The virtual simulation gives both the opportunity of reproducing real indoor/outdoor scenes, but also randomizing the background with artificially generated textures.

Even though the approach appears to obtain sub-optimal results, a complete and adaptable implementation requires a lot of effort, yet unlocking a huge number of possibilities. Considering the consistent amount of fine details to consider and issues that can arise during the development of such simulators, we opt instead to work with an easier generalization pipeline, that mostly concerns machine learning only.




\section{Human Detection and Segmentation}
\label{sec:sota-humandetseg}

\textbf{TODO}



\subsection{Chroma key}

A known approach for implementing background replacement is the chroma key. Widely used in entertainment, it is a technique that makes use of colors in images and videos for splitting between actual content and background. Usually, a chroma key is achieved through a blue or green screen placed behind the subject, making sure that such color is not present in the foreground image. Then, a post-production software takes care of creating the appropriate mask which separates the two parts and enables background replacement.

Although this technique is particularly popular in many fields, placing several green screens into the drone arena is not the easiest task since it requires a lot of material and physical work to set up the proper environment, with possible issues related to the room composition or its illumination.

An experiment in this direction has been conducted during development of Mantegazza et al. \cite{mantegazza2019visionbased}, by placing a green screen on a portion of the arena walls. Masking results were actually satisfying, but the limitations of such a small green screen are huge both in terms of user's movements and background coverage. In fact, figure \ref{fig:greenscreen} displays the setup, which reveals a lot of classic background still appearing in the images.

In addition, even with the capability of building a well-designed chroma key environment, the solution would be highly dependent on the geographical location of the setup. On the contrary, software-only approaches would be much more portable and reusable together with any other motion capture system in the world.

\begin{figure}[!h]
	\begin{center}
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-greenscreen-1"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-greenscreen-2"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-greenscreen-3"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-greenscreen-4"}
		\end{subfigure}
	\end{center}
	\vspace{-0.5cm}
	\caption[Experimental green screen setup in the drone arena]{Experimental green screen setup in the drone arena}
	\label{fig:greenscreen}
\end{figure}



\subsection{Mask R-CNN}
\label{subsec:sota-maskrcnn}

\textbf{TODO}

MaskRCNN: \cite{he2018mask}, \cite{maskrcnn_explanation}, \cite{maskrcnn_arcgis}





