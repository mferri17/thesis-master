\chapter{Background and Related Work}
\label{chap:theory}

\glsresetall

%\lipsum[1]



\section{Theoretical Foundation}

\subsection{3D Poses in Robotics}
\label{subsec:robot-pose}


\subsection{Supervised Learning}
\label{subsec:supervised-learning}


\subsection{Metrics for Regression}
\label{subsec:metrics}

The following list presents an overview of all the principal metrics used in regression. These have been used during the model evaluation in chapter \ref{chap:evaluation}.

\begin{itemize}
	\item \gls{mae} measures the average residuals in the dataset, which means the absolute difference between the actual and predicted values. The lower the \gls{mae}, the better the model. Assuming $y_i$ as the predicted value and $\hat{y_i}$ as the ground truth, the \gls{mae} is defined as follows:
	$$ MAE = \frac{1}{N} \sum_{i=1}^N |y_i - \hat{y_i}| $$
	
	\item \gls{rmse} measures the standard deviation of residuals. As for \gls{mae}, \gls{rmse} should be as lower as possible. It penalizes large prediction errors, thus is not particularly suited to be used on a dataset with many outliers. The main advantage and reason for its usage in evaluating regression models is because \gls{rmse} has the same unit as the dependent variable, so it is easy to interpret.
	$$ RMSE = \sqrt{\frac{1}{N} \sum_{i=1}^N (y_i - \hat{y_i})^2} $$
	
	\item \gls{r2} (or coefficient of determination) measures the robustness of the regression. It represents the proportion of the variance in the dependent variable which is explained by the independent variable. \gls{r2} is often the best choice for evaluating regression performance because it provides a measure of fitness for the model to predict unseen data correctly. Also, its domain is easily understandable since it goes from $-\infty$ to 1: an optimal model (with no prediction errors) will have an \gls{r2} of 1; a model that always predicts the average value will have an \gls{r2} of 0; a model worse than the average predictor will have a negative \gls{r2}. Considering $\overline{y}$ the mean value:
	$$ R^2 = 1 - \frac{\sum_{i=1}^N (y_i - \hat{y_i})^2}{\sum_{i=1}^N (y_i - \overline{y})^2} $$
\end{itemize}


%\subsection{Convolutional Neural Networks (CNN)}
%\label{subsec:cnn}

\subsection{Residual Neural Networks (ResNet)}
\label{subsec:resnet}




\section{Artificial Intelligence for Drones}
\label{sec:human-drone-interaction}

\gls{uav}, commonly known as drones, attracted a lot of interest in the last years. Their usage is expected to raise due to decreasing costs and emerging applications for personal, commercial, and social use. As the sector grows, many methodologies are under development for the control and coordination of drones. Most of the recent works in the field propose artificial intelligence solutions for a large variety of tasks.

In this section, we first present a general overview on recent studies for autonomous flight. Then, we shift our attention on the topic of human-drone interaction and we focus on related works by \gls{idsia}.



\subsection{Autonomous Flight}
\label{subsec:drone-auto-drive}

Autonomous navigation is an important and well-known research area in robotics, which usually requires accomplishing complex and computationally-expensive tasks such as localization, mapping, and path planning. Recent studies have started to approach autonomous driving through imitation learning \cite{imitation_learning_survey}. The approach consist in making a neural network learn a certain task by imitating humans' or robots' behavior on the given assignment.

In 2016, a conjoint team from \gls{idsia} and \gls{uzh} proposes a machine learning approach for the problem of perceiving forest or mountain trails \cite{giusti2016machine}. They use a \gls{dnn} as a supervised image classifier for understanding the direction of a trail, viewed from a camera placed on top of a robot following the path itself. The solution outperforms previous alternatives in the task, making the drone able to autonomously drive following a tipical forest trail. Quantitative results show an accuracy which is comparable to the accuracy of humans on the same classification task.

Some years later, researchers from \gls{uzh} have demonstrated that \gls{resnet}s can achieve satisfactory performance for drones' autonomous driving in indoor and outdoor scenarios. They developed DroNet \cite{Loquercio_2018}, a forked \gls{cnn} that predicts a steering angle and a collision probability from given a gray-scale image. The model learns to steer and avoid obstacles from forward-looking videos recorded by cars and bikes while driving in real contexts. DroNet requires very little computational power, allowing real-time performance, even on a CPU. In this case, the predictions run off-drone on a dedicated computer, remotely connected through WiFi.



\subsection{The State of the Art of Humanâ€“Drone Interaction}
\label{subsec:human-drone-sota}

A good variety of research can be found on human-robot interaction, and a lot is yet to come. In the field, drones represent a specific segment due to their ability to freely move in the 3D space, opening access to new use cases while representing a real challenge for professionals and researchers.

An article published in November 2019 for the IEEE Access \cite{human-drone-sota} explores literature and state of the art in the field of human-drone interaction. It reports that \gls{faa} expects drones' registrations to increase by more than 60\% between 2018 and 2022. The main increment will be seen in the commercial sector, rather than the hobbyist one, even though the latter still counts the large majority of units. Nowadays, almost half of drone usage is for aerial photography (48\%), followed by industrial inspection (28\%) and agriculture (17\%). In the next decade, the authors argue that drones will become ubiquitous to society and extensively used in advertising, shipping, sports, emergency, and many other fields for augmenting human capabilities.

Providing a nice overview of the latest works, the article highlights the most important challenges of today's research on the subject. The main studies in human-drone interaction treat drones' control and human-machine communication. The principal concerns regard the users' perception of safety during flight. These topics are strictly related to our task, concerning autonomous flight in human proximity.



\subsection{Vision-based Control of a Quadrotor in User Proximity}
\label{subsec:sota-dario}

Our work is based on the master thesis \cite{mantegazza2018thesis} and paper \cite{mantegazza2019visionbased} from Dario Mantegazza, named \textit{Vision-based Control of a Quadrotor in User Proximity: Mediated vs. End-to-End Learning Approaches}, developed at \gls{idsia} between 2018 and 2019.

\medskip

In his thesis, the author proposes a machine learning technique for teaching a model to control a drone while interacting with a person. In a few words, the goal is to continuously fly the drone to hover in front of a moving user. The problem is approached as a reactive control procedure and addressed with supervised learning. Thus, it provides an interesting starting point for many other robotics applications. Training data is acquired by programmatically flying the drone to face a user frontally. In this phase, the quadrotor is controlled through an omniscient controller that knows both the drone's and the person's pose. Images produced by the front-facing camera of the drone are used as input for a custom-designed \gls{resnet} architecture that infers the relative user's position \gls{wrt} the drone. The neural network performs a regression on the four variables that form the user's pose (X, Y, Z, YAW) and learns to predict their values using spatial information in the input images.

In the paper, the author compares the mediated approach described above with another end-to-end approach based on imitation learning. This second solution directly learns control signals\footnote{desired pitch, roll, yaw, and vertical velocity} for the drone, instead of the user's pose. A third experiment also considers the task of automatically learning a controller that drives the drone based on the given user's position. All the solutions provide very similar results but, for this thesis, we will only take into consideration the mediated approach. The reason behind our choice is that the learned model can be adapted to other tasks by simply designing a custom controller for the drone. Furthermore, it also provides a more transparent and analyzable solution.

Even though this kind of problem on human recognition and pose estimation could be faced with more advanced deep learning algorithms, making a simple regression on four variables allows the network to be fairly small so that the prediction task is light, fast to execute, and possibly portable on low-end devices. We will address this topic in section \ref{subsec:sota-nicky}.

\medskip

Our entire project makes use of the original network architecture and dataset defined in \cite{mantegazza2019visionbased}, respectively explored in sections \ref{subsec:frontalnet-architecture} and \ref{sec:dataset}. For a better understanding, the original code repository\footnote{\url{https://github.com/idsia-robotics/proximity-quadrotor-learning}} and a descriptive video\footnote{\url{https://drive.switch.ch/index.php/s/MlEDrsuHcSl5Aw5}} are also available. To enhance readability of this thesis, having no official name, the custom \gls{resnet} architecture proposed by the author will be called \textit{FrontalNet}.


\subsubsection{FrontalNet Architecture}
\label{subsec:frontalnet-architecture}

The network comprises a total of 1'332'484 trainable parameters, accepts a single $60 \times 108$ pixels image in input, and outputs 4 regression variables that correspond to the user's pose coordinates. Each \gls{resnet} block is provided with batch normalization, ReLU activations \cite{act-relu} are used for all layers except for the output neurons, which are associated with a linear activation function \cite{act-linear}. Figure \ref{fig:frontalnet-architecture-1} provides an illustration of the mediated architecture we consider for this thesis\footnote{take a look at image \ref{fig:frontalnet-architecture-paper3approaches} for details about the architectures related to the other approaches}. A complete list of all layers is also available in figures \ref{fig:frontalnet-architecture-3a} and \ref{fig:frontalnet-architecture-3a} of the appendix. 

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.4\textwidth]{"contents/images/03-frontalnet-1A"}
	\caption[Schematic FrontalNet architecture]{Schematic FrontalNet architecture \cite{mantegazza2019visionbased}}
	\label{fig:frontalnet-architecture-1}
\end{figure}


\subsubsection{FrontalNet Performance}
\label{subsec:frontalnet-performance}

\glsreset{r2}

FrontalNet is trained using the \gls{mae} loss function with the \gls{adam} optimizer \cite{kingma2014adam} and a base learning rate of 0.001, progressively reduced on validation loss plateaus that last more than 5 epochs. A maximum of 200 epochs are run in total, using an early stopping policy with a patience of 10 epochs on the validation loss. 

Performance is evaluated quantitatively and qualitatively on the end-to-end model, rather than the mediated one considered for our work. However, as explained before, both approaches obtain similar results. We can accordingly consider the following evaluation to be valid for the mediated approach too.

\bigskip 

For quantitative evaluation, the chosen metric is \gls{r2}, whose interpretation is described in section \ref{subsec:metrics}. The author also experiments with finding the minimum cardinality of the dataset to obtain acceptable performance. Results are available in figure \ref{fig:frontalnet-r2}, directly taken from the paper. As shown, at least 5,000 samples are required to achieve decent performance, which improves as the size of the training set increases. Specifically, predictions seem more accurate for variables \texttt{Z} and \texttt{W} with an \gls{r2} score of 0.82 and 0.88, respectively. Quite different the results for \texttt{X} and \texttt{Y} which only reach an \gls{r2} of 0.59 and 0.57, respectively.

These considerations on the variables are confirmed by the qualitative evaluation, obtained by comparing ground truth and predictions during a short simulation. Figure \ref{fig:frontalnet-gt-pred} shows that \texttt{X} and \texttt{Y} predictions are considerably worse than results achieved by \texttt{Z} and \texttt{W} when compared with the ground truth.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=1\textwidth]{"contents/images/03-frontalnet-r2"}
	\caption[FrontalNet quantitative evaluation: \gls{r2} results \cite{mantegazza2019visionbased}]{FrontalNet quantitative evaluation: \gls{r2} results \cite{mantegazza2019visionbased}. A1, A2 and A3 in the chart stands for different models, but they anyway achieve almost the same results.}
	\label{fig:frontalnet-r2}
\end{figure}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.85\textwidth]{"contents/images/03-frontalnet-gt-pred-2"}
	\caption[FrontalNet qualitative evaluation: GT vs. prediction results \cite{mantegazza2019visionbased}]{FrontalNet qualitative evaluation: GT vs. prediction results \cite{mantegazza2019visionbased}. A1, A2 and A3 in the chart stands for different models, but they anyway achieve almost the same results.}
	\label{fig:frontalnet-gt-pred}
\end{figure}


\subsection{Embedded Implementation of Controller for Nano-Drones}
\label{subsec:sota-nicky}

In recent years, both industry and research are starting to work with nano-drones. Their minimal size facilitates indoor flights, even in close proximity to humans. For this reason, they are particularly suited for the exploration of innovative cases on human-drone interaction.

\medskip

In 2019, researchers from \gls{eth} developed PULP-DroNet \cite{palossi2019pulpdronetIoTJ} by porting the DroNet \cite{Loquercio_2018} model (explained in section \ref{subsec:drone-auto-drive}) on the Crazyflie\footnote{\url{https://www.bitcraze.io/products/crazyflie-2-1/}}. This is a nano-drone with a size of only $3 \times 3$ centimeters for a weight of 27 grams. The authors propose a general methodology for deploying on-board deep learning algorithms for ultra-low-power devices without needing an external laptop to run the software.

Inspired by PULP-DroNet, \gls{idsia} adapted its FrontalNet \cite{mantegazza2019visionbased} to work on-board the Crazyflie with encouraging results \cite{zimmerman2020thesis}. The nano-drone can achieve good quantitative and qualitative performance, regardless of the issues deriving from working with such low-end devices. The main challenges highlighted by \cite{zimmerman2020thesis} are found in the need of managing low computational capabilities, energy consumption, and low-fidelity camera images acquired with no video stabilization.




\section{Network Interpretability}
\label{sec:network-interpretability}

\glsreset{ml}
\glsreset{nn}

A \gls{nn} learns abstract representations for finding a mapping between its input and output, determined by well-defined mathematical computations that involve the input itself and the progressively learned network parameters. Inspired by biological brains, this approach seems to be incredibly effective on a huge variety of tasks. Nevertheless, unlike other \gls{ml} techniques, \gls{nn} are known to produce "black-box" models. Their reasoning and comprehension are intrinsic in the network parameters, which are nothing but numbers, particularly hard to understand even from domain experts. 

However, when working with real-world problems, it is crucial to produce explainable results. A thorough understanding of \gls{ml} methods builds trust in algorithms, and makes sure there are no undesirable biases in the models. Otherwise, serious problems can arise especially in critical fields such as medicine and law. \gls{xai} is the field of study which tries to make \gls{ml}, and its underlying basis for decision-making, properly understandable to humans \cite{xai-wiki}.

In Computer Vision, researchers have developed some techniques for understanding what a \gls{cnn} considers for producing an output based on an input image. Primary efforts regard feature visualization and attribution. This section briefly explains these two major areas for \gls{cnn} interpretability, with a particular focus on spatial attribution, the chosen methodology for our work.



\subsection{Feature Visualization}
\label{subsec:feature-vis}

\glsreset{cnn}

Feature visualization aims to represent the way a \gls{cnn} encode its underlying knowledge through a graphical representation of its features. This is done by generating abstract images that explain what the model is really looking for in the input. 
%The technique finds its roots in 2009 \cite{erhan2009feature} and slowly evolves thanks to the effort of many researchers in the field. A recent paper from Olah et al. \cite{olah2017feature} provides a clear explanation on the subject, highlighting its main usage and issues.

A common technique for implementing feature visualization is by maximizing the activation functions of the network through an optimization algorithm \cite{erhan2009feature} \cite{nguyen2016multifaceted}. Such method can be applied on many parts of a \gls{cnn} for understanding individual features: neurons, channels, layers, or even class probabilities. As a result, the algorithm finds a set of input images that is able to effectively maximize the network's output. For example, in a classification problem, feature visualization is able to produce extremely positive examples for a certain class. Please also note that the same features can be associated with different examples in the dataset, as shown in figure \ref{fig:feature-visual-1}. Furthermore, through this technique researchers have demonstrated \cite{olah2017feature} that the layers of a \gls{cnn} build their reasoning in a hierarchical organization. Early layers learn basic visual properties such as edges or textures, while final layers respond to more abstract properties such as patterns, parts, or objects.

Despite being an interesting area of research, feature visualization alone is not enough to produce a satisfactory understanding. Instead, the technique is more prone to be effectively used in the field of \gls{xai} when combined with other tools to explain the system as a whole \cite{zurowietz2020intvis} \cite{olah2018the}.

\begin{figure}[!htb]
	\begin{center}
		\begin{subfigure}[h]{0.23\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/02-feature-bird-2"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.23\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/02-feature-ball-2"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.23\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/02-feature-dog-2"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.23\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/02-feature-cat-2"}
		\end{subfigure}
		\begin{subfigure}[h]{0.23\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/02-feature-bird-1"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.23\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/02-feature-ball-1"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.23\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/02-feature-dog-1"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.23\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/02-feature-cat-1"}
		\end{subfigure}
	\end{center}
	\vspace{-0.5cm}
	\caption[Feature visualization: layer-level features for certain dataset example]{Feature visualization: layer-level features (top) for certain dataset examples (bottom).}
	\label{fig:feature-visual-1}
\end{figure}



\subsection{Spatial Attribution: GradCAM}
\label{subsec:gradcam-theory}

Feature visualization gives an abstract representation on the feature learned by a \gls{cnn}, hence what it is looking for inside an image. However, it does not provide evidence on how or why individual pieces of the network make a final decision. Instead, spatial attribution is capable of explaining the relationship between neurons by highlighting the specific parts of an input image that are most responsible for a certain response of the model \cite{olah2018the}. These methods usually produce a heatmap that overlayed on the input images gives an easy interpretable human-ready visualization.

The methodology is younger yet more powerful than feature visualization. First studies in the field started in 2014, trying to determine most relevant parts in the input images by partially and iteratively occluding portions of them, in search of significant changes in the network output \cite{zeiler2013visualizing}. Later, researchers started to approach the problem from a mathematical perspective by computing saliency maps through backpropagation, back to the input images \cite{simonyan2014deep} or just until the last convolutional layers \cite{zhou2015learning}. Among these techniques, we select Grad-CAM \cite{Selvaraju_2019} for its clean and understandable visualizations.

\medskip

We explain the methodology by first focussing on \gls{cam} \cite{zhou2015learning}, which applies on classification tasks. \gls{cam} requires of a specific \gls{cnn} architecture, composed as follows: a \gls{gap} layer\footnote{The \gls{gap} layer is used for dimensionality reduction, by transforming $h \times w \times d$ feature maps to a single-dimension array of $1 \times 1 \times d$ features \cite{gap_layers}} placed after convolution layers outputs the spatial average of the feature maps; the \gls{gap} output goes into a fully-connected layer; this computes the final classification output through a weighted sum of features values.

Taking inspiration from the forward pass, \gls{cam} projects back the weights of the output layer and computes a weighted sum between them and the feature maps of the last convolutional layer. The operation practically combines multiple activations in a single heatmap that identifies the importance of different parts of an input image for a certain classification output. Figure \ref{fig:cam-schema} illustrates the algorithm. 

\medskip

\gls{gradcam} is a generalization of \gls{cam} which works for any network architecture. The main innovation is that \gls{gradcam} projects back to the network, instead of the last layer's weights, the gradients of the score (for a certain class) \gls{wrt} the feature map activations of a convolutional layer. These gradient are averaged pooled - that is why the \gls{gap} layer is not required anymore - and, as before, weighted sum with the convolutional activation maps. If \gls{gradcam} is applied on a \gls{cam}-compatible network architecture, than it reduces to \gls{cam} itself.

The result of both the algorithms is a heatmap that overlays the regions of an input image responsible for producing a certain model's output. Figure \ref{fig:gradcam-catdog} clearly show the result on a classifier which aims to distinguish cats from dogs.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.9\textwidth]{"contents/images/02-cam"}
	\caption[\gls{cam} schematic functioning]{\gls{cam} schematic functioning \cite{zhou2015learning}}
	\label{fig:cam-schema}
\end{figure}

% \begin{figure}[!htb]
% 	\centering
% 	\includegraphics[width=0.9\textwidth]{"contents/images/gradcam/02-gradcam-schema"}
% 	\caption[\gls{gradcam} schematic functioning]{\gls{gradcam} schematic functioning \cite{Selvaraju_2019}}
% 	\label{fig:gradcam-schema}
% \end{figure}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\textwidth]{"contents/images/gradcam/02-gradcam-catdog"}
    \caption[\gls{gradcam} example on dog vs. cat classification]{\gls{gradcam} example on dog vs. cat classification \cite{Selvaraju_2019}}
    \label{fig:gradcam-catdog}
\end{figure}




\section{Network Generalization}
\label{sec:network-generalization}

This section explores common methodologies for improving neural networks' generalization capabilities. The explained approaches have been selected for specifically introducing our generalization strategy and providing alternative solutions, evaluated and discarded during the development of the thesis.



\subsection{Data Augmentation}
\label{subsec:data-augmentation}

One of the most common approaches for reducing overfitting of a \gls{ml} model is data augmentation. When working with datasets for Computer Vision, we speak about image augmentation. It aims to generate new images from the existing training samples. The technique is ubiquitously used for training \gls{cnn}s, especially when the available data is limited or difficult to obtain. A huge variety of approaches have been proposed by researchers for achieving image augmentation \cite{shorten2019augmentationsurvey}. The most common solutions apply geometric transformations \cite{xie2020unsupervised}, which can be divided in two types:
\begin{itemize}
	\item Spatial-level augmentations: also called affine transformations, relocate pixels by cropping, scaling, rotating, translating, mirroring, and shearing the images. These techniques require to accordingly modify additional elements associated with the images, such as ground truth, masks, bounding boxes, or key points.
	\item Pixel-level augmentations: algorithms that only modify pixels (or group of pixels) without relocating them or changing the image shape. These transformations leave unchanged any other additional element associated with the images, such as ground truth, masks, bounding boxes, or key points.
\end{itemize}

Another simple technique is inspired by the neural networks' mechanism of dropout regularization. It consists of erasing parts of the training images to contrast samples in which the subject is not totally visible. Also, it makes sure that the model actually looks at the entire image while learning its task \cite{wan2013dropconn} \cite{zhong2017random}. 

Fancier methods consider the possibility of mixing multiple samples to create a new image. In \cite{Takahashi_2020} and \cite{summers2019improved}, training images are created as a collage of other randomly transformed samples. In \cite{Lemley_2017}, the authors propose to create a combination of images from the same classification category; the result is similar to the intermediate outputs obtained during the application of a morphing effect \cite{wiki_morphing}. In the last years, the field is experiencing the development of \gls{ml} approaches for automatically learning image augmentation strategies \cite{zoph2019learning} \cite{cubuk2019autoaugment}.

Recent techniques of image augmentation are used to transfer a learned task from one domain into another. In other words, a model trained on images created or augmented in a simulation environment can be later used for achieving the same task in the real world. These techniques exploit the concept of Domain Randomization.



\subsection{Domain Randomization}
\label{subsec:domain-randomization}

Domain Randomization is a trending technique for enabling domain transfer. It is useful when the real-world data is not easy to access and collect \cite{mehta2019active}. The main implementations make use of a simulated environment for representing a virtual world in which the \gls{ml} is trained. If the simulator is good enough in providing a good variety of realistic data, then the real world may appear to the model as just another variation of the training environment. The approach is particularly used for reinforcement learning \cite{imitation_learning_3d_navigation} and object recognition \cite{weng2019DR} \cite{tobin2017domain}.

Entire tasks, including environment and moving agents, can be simulated through a dedicated graphical engine. Some of the most important are Gazebo\footnote{\url{http://gazebosim.org/}}, Unreal Engine\footnote{\url{https://www.unrealengine.com/en-US/}}, and Unity\footnote{\url{https://unity.com/}}. Regardless of being designed specifically for Robotics or for generic purposes, such as game-development or \gls{vr}, all of them give developers unlimited possibilities. However, a complete and adaptable simulation of a machine learning task with physics engines requires a lot of effort.

For this reason, recent studies consider advanced image augmentation techniques for achieving Domain Randomization. In \cite{yue2019domain}, the authors propose an interesting and powerful approach for augmenting a dataset using artistic style transfer \cite{gatys2015neural}. They modify an input sample by applying the style of an image taken from another database. The results produce curious synthetic images which give to the training model enough variability to learn a generalized self-driving task. Figure \ref{fig:02-domain-artistic} shows an example of this application.

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\textwidth]{"contents/images/02-domain-artistic"}
    \caption[An example of artistic style transfer for Domain Randomization]{An example of artistic style transfer for Domain Randomization \cite{yue2019domain}}
    \label{fig:02-domain-artistic}
\end{figure}



\subsection{Background Replacement: Chroma Key}

In chapter \ref{chap:system}, we provide an overview of the system by Mantegazza et al. \cite{mantegazza2019visionbased}, presented in section \ref{subsec:sota-dario}. As previously discussed, the images for training the model have been recorded in a single room. Inspired by Domain Randomization, we focus on designing a custom image augmentation technique to simulate a larger variety of environments. More specifically, we want to replace the background behind each user in the dataset.

\medskip

A known approach for implementing background replacement is the chroma key. Widely used in entertainment, it is a technique that makes use of colors in images and videos for creating a mask that distinguishes between actual content and background. Usually, a chroma key is implemented using a blue or green screen placed behind the subject, making sure that such color is not present in the foreground image. Then, a post-production software takes care of creating the mask of the subject, enabling the background replacement.

Building up a proper chroma key setup can be challenging, especially in large environments. An experiment in this direction has been conducted during the development of \cite{mantegazza2019visionbased}, by placing a green screen on a portion of the drone arena walls. Masking results were actually satisfying but the limited size of the screen was suffering from limitations in terms of user's movements and background coverage. Figure \ref{fig:greenscreen} displays the setup, revealing a non-ignorable portion of the original background still appearing in the images. Besides, assuming the capability of building a well-designed chroma key environment, the resulting solution would be highly dependent on the setup's geographical location.

For this reason, we decide to focus on solutions that only regard software computations. They are much more flexible and open the possibility of extending the same approach for other tasks.

\begin{figure}[!h]
	\begin{center}
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-greenscreen-1"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-greenscreen-2"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-greenscreen-3"}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.24\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{"contents/images/04-greenscreen-4"}
		\end{subfigure}
	\end{center}
	\vspace{-0.5cm}
	\caption[Experimental green screen setup in the drone arena]{Experimental green screen setup in the drone arena}
	\label{fig:greenscreen}
\end{figure}



\subsection{Human Detection and Segmentation: Mask R-CNN}
\label{subsec:sota-maskrcnn}

Mask R-CNN \cite{he2018mask} is a state of the art deep learning framework for object detection and instance segmentation. In other words, it detects and categorizes every single object inside an image and also creates the corresponding mask.

The method extends Faster R-CNN \cite{ren2016faster} by adding a branch in the network for simultaneously predicting the object mask while the existing model cares of bounding box recognition. Faster R-CNN consists of two steps. The first is based on a Region Proposal Network, a Fully Convolutional Network \cite{shelhamer2016fully} which proposes candidate object bounding boxes. The second fundamental stage extract squared features from each candidate to perform classification and bounding-box regression. In Mask R-CNN, the first step remains the same, but the second stage also includes a parallel prediction of a binary mask.
Results are qualitatively incredible\footnote{\url{https://www.youtube.com/watch?v=OOT3UIXZztE}, \url{https://youtu.be/Dhkd_bAwwMc}} and the method currently outperforms any other study in the field. A practical demo applied to our dataset is available in section \ref{subsec:masking-maskrcnn}.

Please note that Faster R-CNN is much slower \cite{medium_rcnn} than other object detection algorithms, such as YOLO \cite{redmon2016look}. However, the strength of Mask R-CNN stays in its high-precision in providing both object detection and instance segmentation at once. Furthermore, the framework is easily adaptable for various segmentation and keypoint detection tasks \cite{he2018mask} \cite{MaskRCNN_matterport} \cite{maskrcnn_explanation}, and it destined to become hugely used in many fields of application.





